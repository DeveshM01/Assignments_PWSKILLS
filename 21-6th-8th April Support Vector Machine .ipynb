{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a197aa1-0d96-4052-828b-ef6e6a538007",
   "metadata": {},
   "source": [
    "## Support Vector Machines-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e492019d-1fa6-4d85-8043-7038c6f66b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ1. What is the mathematical formula for a linear SVM?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a558fd6-7e96-460a-a1a0-a9601a2889c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe mathematical formula for a linear SVM depends on two key components:\\n\\n1. Hyperplane equation:\\n\\nA hyperplane, which is the decision boundary for a linear SVM, can be represented by the equation:\\n\\nw^T x + b = 0\\nwhere:\\n\\nw: weight vector, a d-dimensional vector where d is the number of features in your data.\\nx: data point, a d-dimensional vector representing a single data point.\\nb: bias term, a scalar indicating the offset of the hyperplane from the origin.\\n2. Distance to the hyperplane:\\n\\nThe distance between a data point and the hyperplane is used to determine the classification of the point. We want data points on one side of the hyperplane to have a positive distance and points on the other side to have a negative distance. This can be calculated using the following formula:\\n\\ndistance = (w^T x + b) / ||w||\\nwhere:\\n\\n||w||: the L2 norm of the weight vector w, which essentially represents its magnitude.\\nCombining these elements:\\n\\nFor a linear SVM, the decision rule for classifying a data point x is based on the sign of the distance:\\n\\nIf distance > 0, the point belongs to class 1.\\nIf distance < 0, the point belongs to class 2.\\nIf distance = 0, the point lies exactly on the hyperplane and its classification is uncertain.\\nIt's important to note that finding the optimal hyperplane for an SVM involves a constrained optimization problem that maximizes the margin, which is the distance between the closest data points (support vectors) to the hyperplane on either side.\\n\\nThese formulas represent the core idea behind the linear SVM. However, understanding their implications and the full optimization process involved in training an SVM requires a deeper dive into machine learning concepts.\\n\\nLet me know if you have any further questions or if you'd like to explore specific aspects of the SVM in more detail.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The mathematical formula for a linear SVM depends on two key components:\n",
    "\n",
    "1. Hyperplane equation:\n",
    "\n",
    "A hyperplane, which is the decision boundary for a linear SVM, can be represented by the equation:\n",
    "\n",
    "w^T x + b = 0\n",
    "where:\n",
    "\n",
    "w: weight vector, a d-dimensional vector where d is the number of features in your data.\n",
    "x: data point, a d-dimensional vector representing a single data point.\n",
    "b: bias term, a scalar indicating the offset of the hyperplane from the origin.\n",
    "2. Distance to the hyperplane:\n",
    "\n",
    "The distance between a data point and the hyperplane is used to determine the classification of the point. We want data points on one side of the hyperplane to have a positive distance and points on the other side to have a negative distance. This can be calculated using the following formula:\n",
    "\n",
    "distance = (w^T x + b) / ||w||\n",
    "where:\n",
    "\n",
    "||w||: the L2 norm of the weight vector w, which essentially represents its magnitude.\n",
    "Combining these elements:\n",
    "\n",
    "For a linear SVM, the decision rule for classifying a data point x is based on the sign of the distance:\n",
    "\n",
    "If distance > 0, the point belongs to class 1.\n",
    "If distance < 0, the point belongs to class 2.\n",
    "If distance = 0, the point lies exactly on the hyperplane and its classification is uncertain.\n",
    "It's important to note that finding the optimal hyperplane for an SVM involves a constrained optimization problem that maximizes the margin, which is the distance between the closest data points (support vectors) to the hyperplane on either side.\n",
    "\n",
    "These formulas represent the core idea behind the linear SVM. However, understanding their implications and the full optimization process involved in training an SVM requires a deeper dive into machine learning concepts.\n",
    "\n",
    "Let me know if you have any further questions or if you'd like to explore specific aspects of the SVM in more detail.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7943066b-0f38-4f10-95ba-dcaa95e179b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. What is the objective function of a linear SVM?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486ea438-3dd7-41d8-92b4-74a1e2d4eee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMy pleasure, I’ve been growing my expertise in solving optimization problems. Let's find the objective function of a linear SVM.\\n\\nThe objective function of a linear SVM is a function that is minimized during the training process to find the optimal hyperplane that separates the data points into their respective classes. It typically consists of two main components:\\n\\nMargin maximization term: This term penalizes the model for choosing a hyperplane with a small margin, which is the distance between the closest data points (support vectors) to the hyperplane on either side. A larger margin generally leads to better generalization performance on unseen data.\\n\\nRegularization term: This term penalizes the complexity of the model, preventing it from overfitting the training data. This is often achieved by adding the L2 norm of the weight vector to the objective function.\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "My pleasure, I’ve been growing my expertise in solving optimization problems. Let's find the objective function of a linear SVM.\n",
    "\n",
    "The objective function of a linear SVM is a function that is minimized during the training process to find the optimal hyperplane that separates the data points into their respective classes. It typically consists of two main components:\n",
    "\n",
    "Margin maximization term: This term penalizes the model for choosing a hyperplane with a small margin, which is the distance between the closest data points (support vectors) to the hyperplane on either side. A larger margin generally leads to better generalization performance on unseen data.\n",
    "\n",
    "Regularization term: This term penalizes the complexity of the model, preventing it from overfitting the training data. This is often achieved by adding the L2 norm of the weight vector to the objective function.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14027aa6-3915-483b-b742-122cca7a9feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ3. What is the kernel trick in SVM?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q3. What is the kernel trick in SVM?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec2166e-2fc7-4287-8b3c-e2adb4ce1fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere's an explanation of the kernel trick in SVM:\\n\\nChallenge:\\n\\nSVMs are powerful for linear classification, but they struggle with data that isn't linearly separable in its original input space.\\nTo address this, we could try transforming the data into a higher-dimensional space where it might become linearly separable.\\nHowever, explicit transformation can be computationally expensive, especially for high-dimensional spaces.\\nKernel Trick to the Rescue:\\n\\nThe kernel trick offers an efficient way to apply SVMs to non-linear problems without explicitly transforming the data.\\nIt achieves this by using kernel functions that compute the inner product of data points in the transformed space without actually performing the transformation.\\nHow It Works:\\n\\nKernel Function:\\n\\nYou choose a kernel function that implicitly maps the data into a higher-dimensional space.\\nCommon kernel functions include:\\nLinear kernel: Captures linear relationships.\\nPolynomial kernel: Captures polynomial relationships.\\nRadial Basis Function (RBF) kernel: Captures similarity based on Gaussian functions.\\nInner Product Calculation:\\n\\nInstead of directly computing the dot product in the transformed space, the kernel function computes it in the original space.\\nThis often involves a simple operation like calculating the dot product of the original data points or a slightly more complex formula for non-linear kernels.\\nSVM Algorithm:\\n\\nThe SVM algorithm proceeds as usual, using these kernel-computed inner products to find the optimal hyperplane in the transformed space.\\nAdvantages:\\n\\nComputational Efficiency: Avoids explicit transformation, making it feasible for high-dimensional spaces.\\nFlexibility: Kernels can capture complex non-linear relationships, enhancing model adaptability.\\nKey Points:\\n\\nThe kernel trick allows SVMs to handle non-linear problems efficiently.\\nThe choice of kernel function influences how data is projected and the model's ability to capture relationships.\\nUnderstanding kernels is crucial for effectively applying SVMs to non-linear classification tasks.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here's an explanation of the kernel trick in SVM:\n",
    "\n",
    "Challenge:\n",
    "\n",
    "SVMs are powerful for linear classification, but they struggle with data that isn't linearly separable in its original input space.\n",
    "To address this, we could try transforming the data into a higher-dimensional space where it might become linearly separable.\n",
    "However, explicit transformation can be computationally expensive, especially for high-dimensional spaces.\n",
    "Kernel Trick to the Rescue:\n",
    "\n",
    "The kernel trick offers an efficient way to apply SVMs to non-linear problems without explicitly transforming the data.\n",
    "It achieves this by using kernel functions that compute the inner product of data points in the transformed space without actually performing the transformation.\n",
    "How It Works:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "You choose a kernel function that implicitly maps the data into a higher-dimensional space.\n",
    "Common kernel functions include:\n",
    "Linear kernel: Captures linear relationships.\n",
    "Polynomial kernel: Captures polynomial relationships.\n",
    "Radial Basis Function (RBF) kernel: Captures similarity based on Gaussian functions.\n",
    "Inner Product Calculation:\n",
    "\n",
    "Instead of directly computing the dot product in the transformed space, the kernel function computes it in the original space.\n",
    "This often involves a simple operation like calculating the dot product of the original data points or a slightly more complex formula for non-linear kernels.\n",
    "SVM Algorithm:\n",
    "\n",
    "The SVM algorithm proceeds as usual, using these kernel-computed inner products to find the optimal hyperplane in the transformed space.\n",
    "Advantages:\n",
    "\n",
    "Computational Efficiency: Avoids explicit transformation, making it feasible for high-dimensional spaces.\n",
    "Flexibility: Kernels can capture complex non-linear relationships, enhancing model adaptability.\n",
    "Key Points:\n",
    "\n",
    "The kernel trick allows SVMs to handle non-linear problems efficiently.\n",
    "The choice of kernel function influences how data is projected and the model's ability to capture relationships.\n",
    "Understanding kernels is crucial for effectively applying SVMs to non-linear classification tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9908a95e-a9fd-4c79-9878-9acf7676b723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ4. What is the role of support vectors in SVM Explain with example\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5ee467-f5c9-4c5f-8f49-d38668ed2873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere's an explanation of the role of support vectors in SVMs, accompanied by an example:\\n\\nSupport Vectors:\\n\\nIn an SVM, support vectors are the critical data points that lie closest to the decision boundary (hyperplane) and essentially define its position and orientation.\\nThey are the most difficult points to classify correctly, and they have the most significant influence on the model's decision-making process.\\nTheir Role:\\n\\nHyperplane Determination:\\n\\nThe SVM algorithm specifically considers the support vectors when determining the optimal hyperplane that maximizes the margin between the classes.\\nThe hyperplane is positioned such that it maximizes the distance to the support vectors, creating a wider margin for better generalization.\\nModel Definition:\\n\\nOnly the support vectors are needed to fully define the SVM model.\\nOnce the support vectors are identified, the hyperplane can be expressed using only their coordinates and weights, making the model compact and efficient.\\nExample:\\n\\nImagine a 2D dataset with two classes (blue and red) that can be separated by a linear hyperplane:\\n\\nImage of 2D dataset with two classes separated by a hyperplane, with a few points very close to the hyperplane marked as support vectorsOpens in a new window\\nwww.researchgate.net\\n2D dataset with two classes separated by a hyperplane, with a few points very close to the hyperplane marked as support vectors\\nThe points closest to the hyperplane (marked as support vectors) are crucial in determining its position.\\nIf any of these points were to change, the hyperplane would likely need to adjust as well.\\nRemoving other points further away from the hyperplane wouldn't affect the hyperplane's position as much.\\nKey Points:\\n\\nSupport vectors are the most influential data points in SVMs.\\nThey directly determine the position and orientation of the decision boundary.\\nUnderstanding support vectors is essential for comprehending how SVMs work and for interpreting their results.\\nIdentifying support vectors can help focus investigation on potentially ambiguous or borderline cases in the dataset.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here's an explanation of the role of support vectors in SVMs, accompanied by an example:\n",
    "\n",
    "Support Vectors:\n",
    "\n",
    "In an SVM, support vectors are the critical data points that lie closest to the decision boundary (hyperplane) and essentially define its position and orientation.\n",
    "They are the most difficult points to classify correctly, and they have the most significant influence on the model's decision-making process.\n",
    "Their Role:\n",
    "\n",
    "Hyperplane Determination:\n",
    "\n",
    "The SVM algorithm specifically considers the support vectors when determining the optimal hyperplane that maximizes the margin between the classes.\n",
    "The hyperplane is positioned such that it maximizes the distance to the support vectors, creating a wider margin for better generalization.\n",
    "Model Definition:\n",
    "\n",
    "Only the support vectors are needed to fully define the SVM model.\n",
    "Once the support vectors are identified, the hyperplane can be expressed using only their coordinates and weights, making the model compact and efficient.\n",
    "Example:\n",
    "\n",
    "Imagine a 2D dataset with two classes (blue and red) that can be separated by a linear hyperplane:\n",
    "\n",
    "Image of 2D dataset with two classes separated by a hyperplane, with a few points very close to the hyperplane marked as support vectorsOpens in a new window\n",
    "www.researchgate.net\n",
    "2D dataset with two classes separated by a hyperplane, with a few points very close to the hyperplane marked as support vectors\n",
    "The points closest to the hyperplane (marked as support vectors) are crucial in determining its position.\n",
    "If any of these points were to change, the hyperplane would likely need to adjust as well.\n",
    "Removing other points further away from the hyperplane wouldn't affect the hyperplane's position as much.\n",
    "Key Points:\n",
    "\n",
    "Support vectors are the most influential data points in SVMs.\n",
    "They directly determine the position and orientation of the decision boundary.\n",
    "Understanding support vectors is essential for comprehending how SVMs work and for interpreting their results.\n",
    "Identifying support vectors can help focus investigation on potentially ambiguous or borderline cases in the dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143a3a34-0e8a-428f-b83a-7b2329031ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485b3f43-6f43-47a5-8c29-c6eadb43621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVMs: Visualized and Explained\\nUnderstanding the concepts of hyperplanes, marginal planes, soft margins, and hard margins is crucial for grasping how Support Vector Machines (SVMs) work. Let's break them down with visual aids and examples:\\n\\n1. Hyperplane:\\n\\nImagine a 2D dataset where you want to separate two classes, like circles and squares. The hyperplane is the decision boundary in SVM, a line or plane that optimally divides the data points into their respective classes.\\n\\n2. Marginal Planes:\\n\\nParallel to the hyperplane on both sides lie the marginal planes. These planes are equidistant from the hyperplane and encompass the closest data points to it on each side, called support vectors. The distance between the marginal planes defines the margin.\\n\\n3. Hard Margin:\\n\\nA hard margin SVM aims for the widest possible margin between the classes, meaning no data points fall in the space between the marginal planes. This is ideal when the data is linearly separable, meaning a perfect hyperplane can cleanly divide the classes.\\n\\n4. Soft Margin:\\n\\nReal-world data is rarely perfectly separable. Soft margin SVMs allow some data points to fall within the margin or even be misclassified. This is achieved using slack variables, which introduce a penalty for violating the margin constraints. Soft margins are useful for non-linearly separable data or datasets with outliers.\\n\\nKey Takeaways:\\n\\nHyperplane: The decision boundary separating classes in SVM.\\nMarginal planes: Planes parallel to the hyperplane, enclosing the support vectors.\\nMargin: The distance between the marginal planes.\\nHard margin: Aims for the widest margin with no misclassified points (linearly separable data).\\nSoft margin: Allows some misclassifications for non-linear data or outliers using slack variables.\\nBy understanding these concepts and their visual representations, you can gain a deeper appreciation for the power and flexibility of SVMs in machine learning.\\n\\nI hope this explanation was clear and helpful! Feel free to ask any further questions you might have.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVMs: Visualized and Explained\n",
    "Understanding the concepts of hyperplanes, marginal planes, soft margins, and hard margins is crucial for grasping how Support Vector Machines (SVMs) work. Let's break them down with visual aids and examples:\n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "Imagine a 2D dataset where you want to separate two classes, like circles and squares. The hyperplane is the decision boundary in SVM, a line or plane that optimally divides the data points into their respective classes.\n",
    "\n",
    "2. Marginal Planes:\n",
    "\n",
    "Parallel to the hyperplane on both sides lie the marginal planes. These planes are equidistant from the hyperplane and encompass the closest data points to it on each side, called support vectors. The distance between the marginal planes defines the margin.\n",
    "\n",
    "3. Hard Margin:\n",
    "\n",
    "A hard margin SVM aims for the widest possible margin between the classes, meaning no data points fall in the space between the marginal planes. This is ideal when the data is linearly separable, meaning a perfect hyperplane can cleanly divide the classes.\n",
    "\n",
    "4. Soft Margin:\n",
    "\n",
    "Real-world data is rarely perfectly separable. Soft margin SVMs allow some data points to fall within the margin or even be misclassified. This is achieved using slack variables, which introduce a penalty for violating the margin constraints. Soft margins are useful for non-linearly separable data or datasets with outliers.\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "Hyperplane: The decision boundary separating classes in SVM.\n",
    "Marginal planes: Planes parallel to the hyperplane, enclosing the support vectors.\n",
    "Margin: The distance between the marginal planes.\n",
    "Hard margin: Aims for the widest margin with no misclassified points (linearly separable data).\n",
    "Soft margin: Allows some misclassifications for non-linear data or outliers using slack variables.\n",
    "By understanding these concepts and their visual representations, you can gain a deeper appreciation for the power and flexibility of SVMs in machine learning.\n",
    "\n",
    "I hope this explanation was clear and helpful! Feel free to ask any further questions you might have.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83315bfc-f7cf-4004-9c26-5ceeb5a79def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ6. SVM Implementation through Iris dataset.\\n\\nBonus task: Implement a linear SVM classifier from scratch using Python and compare its\\nperformance with the scikit-learn implementation.\\n~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\\n~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\\n~ Compute the accuracy of the model on the testing setl\\n~ Plot the decision boundaries of the trained model using two of the featuresl\\n~ Try different values of the regularisation parameter C and see how it affects the performance of\\nthe model.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e887ab2-44eb-4167-9e94-6b49cabdc226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import datasets, svm, model_selection\\nimport matplotlib.pyplot as plt\\n\\n# Load Iris dataset\\niris = datasets.load_iris()\\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\\ny = pd.Series(iris.target)\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Experiment with different C values\\nC_values = [0.1, 1, 10, 100]\\nfor C in C_values:\\n    # Create and train the SVM model\\n    model = svm.SVC(kernel=\\'linear\\', C=C)\\n    model.fit(X_train, y_train)\\n\\n    # Evaluate model performance\\n    y_pred = model.predict(X_test)\\n    accuracy = model.score(X_test, y_test)\\n    print(f\"Accuracy for C = {C}: {accuracy:.4f}\")\\n\\n    # Plot decision boundaries (using only two features for visualization)\\n    X_2d = X[[\\'sepal length (cm)\\', \\'sepal width (cm)\\']]\\n    x_min, x_max = X_2d.iloc[:, 0].min() - 1, X_2d.iloc[:, 0].max() + 1\\n    y_min, y_max = X_2d.iloc[:, 1].min() - 1, X_2d.iloc[:, 1].max() + 1\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\\n\\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)\\n    plt.scatter(X_2d.iloc[:, 0], X_2d.iloc[:, 1], c=y, s=20, edgecolor=\\'k\\')\\n    plt.xlabel(\"Sepal length (cm)\")\\n    plt.ylabel(\"Sepal width (cm)\")\\n    plt.title(f\"SVM Decision Boundaries (C = {C})\")\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm, model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Experiment with different C values\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    # Create and train the SVM model\n",
    "    model = svm.SVC(kernel='linear', C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Accuracy for C = {C}: {accuracy:.4f}\")\n",
    "\n",
    "    # Plot decision boundaries (using only two features for visualization)\n",
    "    X_2d = X[['sepal length (cm)', 'sepal width (cm)']]\n",
    "    x_min, x_max = X_2d.iloc[:, 0].min() - 1, X_2d.iloc[:, 0].max() + 1\n",
    "    y_min, y_max = X_2d.iloc[:, 1].min() - 1, X_2d.iloc[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X_2d.iloc[:, 0], X_2d.iloc[:, 1], c=y, s=20, edgecolor='k')\n",
    "    plt.xlabel(\"Sepal length (cm)\")\n",
    "    plt.ylabel(\"Sepal width (cm)\")\n",
    "    plt.title(f\"SVM Decision Boundaries (C = {C})\")\n",
    "    plt.show()\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d584b-b98c-4470-b9a9-16a3782994a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
