{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f3da5a-5dcd-4b56-8f3e-e74be608404f",
   "metadata": {},
   "source": [
    "19 March Feature Engineering Assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e41bc2-606a-4d9b-b1a7-29555e7afeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa13e9e-bdf0-44ec-b31d-851658089914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Min-Max scaling, also known as Min-Max normalization or feature scaling, is a data preprocessing technique used to scale numerical features of a dataset to a specific range. \\nThe goal is to transform the data into a predefined range, typically [0, 1], where the minimum value of the original data is mapped to 0, and the maximum value is mapped to 1.\\n\\nThe formula for Min-Max scaling is as follows:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Min-Max scaling, also known as Min-Max normalization or feature scaling, is a data preprocessing technique used to scale numerical features of a dataset to a specific range. \n",
    "The goal is to transform the data into a predefined range, typically [0, 1], where the minimum value of the original data is mapped to 0, and the maximum value is mapped to 1.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16d0c29-f488-4d66-9ec5-675694947764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   ],\n",
       "       [0.375],\n",
       "       [1.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "income_data = [[20000], [50000], [100000]]\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "scaled_income_data = scaler.fit_transform(income_data)\n",
    "\n",
    "# Display the scaled data\n",
    "scaled_income_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e72b81-e49b-42cf-95c0-bad9ba841b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5de600-ae96-4d5e-95d6-64c1a8b45e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Unit Vector technique, also known as vector normalization or normalization to unit length, is a feature scaling method that scales each feature of a dataset to have a unit norm.\\nIt focuses on the direction of the data points rather than their magnitude. The goal is to ensure that each data point lies on the surface of a unit hypersphere.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"The Unit Vector technique, also known as vector normalization or normalization to unit length, is a feature scaling method that scales each feature of a dataset to have a unit norm.\n",
    "It focuses on the direction of the data points rather than their magnitude. The goal is to ensure that each data point lies on the surface of a unit hypersphere.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b01562-50bd-424a-a3f7-a0ec6765dbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's how it differs from Min-Max scaling:\\n\\nMin-Max Scaling:\\n\\nScales the data to a specific range, typically [0, 1].\\nFocuses on the magnitude of the data points.\\nUseful when the magnitude of features is important for the machine learning algorithm.\\nUnit Vector Scaling:\\n\\nScales each data point to have a unit norm (length 1).\\nFocuses on the direction of the data points.\\nUseful when the magnitude of features is not important, but the direction or angle between data points matters.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Here's how it differs from Min-Max scaling:\n",
    "\n",
    "Min-Max Scaling:\n",
    "\n",
    "Scales the data to a specific range, typically [0, 1].\n",
    "Focuses on the magnitude of the data points.\n",
    "Useful when the magnitude of features is important for the machine learning algorithm.\n",
    "Unit Vector Scaling:\n",
    "\n",
    "Scales each data point to have a unit norm (length 1).\n",
    "Focuses on the direction of the data points.\n",
    "Useful when the magnitude of features is not important, but the direction or angle between data points matters.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b48e1b-ee0b-4a82-957c-21a9f3c38a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6       ,  0.8       ],\n",
       "       [ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[3, 4], [1, -1], [2, 2]])\n",
    "\n",
    "# Create a Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit the normalizer to the data and transform the data\n",
    "normalized_data = normalizer.fit_transform(data)\n",
    "\n",
    "# Display the normalized data\n",
    "normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c760dc4e-c686-41b7-8037-375183e11d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this example, data is a 2D NumPy array representing a dataset with two features. \\nThe Normalizer is used to normalize the data, and the result is stored in normalized_data. \\nThe output will show the unit vectors corresponding to the original data points, emphasizing the direction of each data point rather than its magnitude.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this example, data is a 2D NumPy array representing a dataset with two features. \n",
    "The Normalizer is used to normalize the data, and the result is stored in normalized_data. \n",
    "The output will show the unit vectors corresponding to the original data points, emphasizing the direction of each data point rather than its magnitude.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e037968-f460-45c0-8470-7de5d08331d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80decaf1-e896-4c39-ab45-e8676555cb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. The primary goal of PCA is to transform a high-dimensional dataset into a new coordinate system, where the data's variance is maximized along the new axes. The new axes are called principal components, and they are orthogonal (uncorrelated) to each other.\\n\\nThe steps involved in PCA are as follows:\\n\\nStandardize the Data:\\n\\nIf the features in the dataset have different scales, it is essential to standardize them to have zero mean and unit variance.\\nCalculate the Covariance Matrix:\\n\\nCompute the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features.\\nCompute Eigenvectors and Eigenvalues:\\n\\nFind the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of the variance along those directions.\\nSort Eigenvectors by Eigenvalues:\\n\\nSort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the principal component with the most variance.\\nSelect Principal Components:\\n\\nChoose the top \\n�\\nk eigenvectors to form a new matrix, where \\n�\\nk is the desired dimensionality of the reduced dataset.\\nProject Data onto New Subspace:\\n\\nMultiply the original data by the matrix of selected eigenvectors to obtain the reduced-dimensional representation of the data.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. The primary goal of PCA is to transform a high-dimensional dataset into a new coordinate system, where the data's variance is maximized along the new axes. The new axes are called principal components, and they are orthogonal (uncorrelated) to each other.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the Data:\n",
    "\n",
    "If the features in the dataset have different scales, it is essential to standardize them to have zero mean and unit variance.\n",
    "Calculate the Covariance Matrix:\n",
    "\n",
    "Compute the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features.\n",
    "Compute Eigenvectors and Eigenvalues:\n",
    "\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of the variance along those directions.\n",
    "Sort Eigenvectors by Eigenvalues:\n",
    "\n",
    "Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the principal component with the most variance.\n",
    "Select Principal Components:\n",
    "\n",
    "Choose the top \n",
    "�\n",
    "k eigenvectors to form a new matrix, where \n",
    "�\n",
    "k is the desired dimensionality of the reduced dataset.\n",
    "Project Data onto New Subspace:\n",
    "\n",
    "Multiply the original data by the matrix of selected eigenvectors to obtain the reduced-dimensional representation of the data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c00f493-9e57-49cf-9556-6f126b1a6303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal Component 1</th>\n",
       "      <th>Principal Component 2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Principal Component 1  Principal Component 2  Target\n",
       "0              -2.264703               0.480027       0\n",
       "1              -2.080961              -0.674134       0\n",
       "2              -2.364229              -0.341908       0\n",
       "3              -2.299384              -0.597395       0\n",
       "4              -2.389842               0.646835       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data (not always necessary, but good practice)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA with 2 components (for illustration purposes)\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Create a DataFrame with the reduced-dimensional data\n",
    "columns = ['Principal Component 1', 'Principal Component 2']\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=columns)\n",
    "df_pca['Target'] = y\n",
    "\n",
    "# Display the reduced-dimensional data\n",
    "df_pca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1149f96-94e2-4e40-a0f8-18adc2bcb66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this example, we use the Iris dataset, which has four features. We apply PCA to reduce the data to two dimensions (2 principal components).\\nThe resulting DataFrame (df_pca) contains the reduced-dimensional representation of the data, making it easier to visualize and analyze.\\nEach row in the DataFrame represents an observation, and the columns represent the values along the principal components. The Target column indicates the class labels'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this example, we use the Iris dataset, which has four features. We apply PCA to reduce the data to two dimensions (2 principal components).\n",
    "The resulting DataFrame (df_pca) contains the reduced-dimensional representation of the data, making it easier to visualize and analyze.\n",
    "Each row in the DataFrame represents an observation, and the columns represent the values along the principal components. The Target column indicates the class labels\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4546d08d-0fd7-4a0d-8537-adf820f8f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330a816a-253c-416a-8004-52da96ae0a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PCA (Principal Component Analysis) is a technique commonly used for dimensionality reduction and feature extraction. The relationship between PCA and feature extraction lies in PCA's ability to transform a high-dimensional dataset into a lower-dimensional space by capturing the most significant variation in the data. The principal components obtained through PCA can be considered as new features that retain the essential information from the original features.\\n\\nHere's how PCA can be used for feature extraction:\\n\\nIdentifying Principal Components:\\n\\nPCA identifies the principal components, which are linear combinations of the original features. These components are ordered by the amount of variance they capture in the data. The first principal component captures the most variance, the second principal component captures the second most, and so on.\\nSelecting a Subset of Principal Components:\\n\\nTo perform feature extraction, you can choose a subset of the top principal components. By selecting a smaller number of principal components, you effectively reduce the dimensionality of the dataset.\\nTransforming the Data:\\n\\nThe original data can be transformed by projecting it onto the selected principal components. This transformation results in a new representation of the data with a reduced number of features.\\nLet's provide an example using Python and the scikit-learn library to demonstrate how PCA can be used for feature extraction:\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PCA (Principal Component Analysis) is a technique commonly used for dimensionality reduction and feature extraction. The relationship between PCA and feature extraction lies in PCA's ability to transform a high-dimensional dataset into a lower-dimensional space by capturing the most significant variation in the data. The principal components obtained through PCA can be considered as new features that retain the essential information from the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Identifying Principal Components:\n",
    "\n",
    "PCA identifies the principal components, which are linear combinations of the original features. These components are ordered by the amount of variance they capture in the data. The first principal component captures the most variance, the second principal component captures the second most, and so on.\n",
    "Selecting a Subset of Principal Components:\n",
    "\n",
    "To perform feature extraction, you can choose a subset of the top principal components. By selecting a smaller number of principal components, you effectively reduce the dimensionality of the dataset.\n",
    "Transforming the Data:\n",
    "\n",
    "The original data can be transformed by projecting it onto the selected principal components. This transformation results in a new representation of the data with a reduced number of features.\n",
    "Let's provide an example using Python and the scikit-learn library to demonstrate how PCA can be used for feature extraction:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6df41881-b546-4c47-b32e-b1e280f38ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal Component 1</th>\n",
       "      <th>Principal Component 2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Principal Component 1  Principal Component 2  Target\n",
       "0              -2.264703               0.480027       0\n",
       "1              -2.080961              -0.674134       0\n",
       "2              -2.364229              -0.341908       0\n",
       "3              -2.299384              -0.597395       0\n",
       "4              -2.389842               0.646835       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data (not always necessary, but good practice)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA for feature extraction (choosing 2 principal components for illustration)\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Create a DataFrame with the extracted features\n",
    "columns = ['Principal Component 1', 'Principal Component 2']\n",
    "df_extracted_features = pd.DataFrame(data=principal_components, columns=columns)\n",
    "df_extracted_features['Target'] = y\n",
    "\n",
    "# Display the DataFrame with the extracted features\n",
    "df_extracted_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db77912b-7f59-452b-9c95-feb6308ea5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this example, we use PCA to extract two principal components from the Iris dataset, which originally has four features. The resulting DataFrame (df_extracted_features) contains the reduced set of features represented by the two principal components along with the class labels. This demonstrates how PCA can be employed for feature extraction, allowing for a more concise representation of the data while preserving its essential characteristics.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this example, we use PCA to extract two principal components from the Iris dataset, which originally has four features. The resulting DataFrame (df_extracted_features) contains the reduced set of features represented by the two principal components along with the class labels. This demonstrates how PCA can be employed for feature extraction, allowing for a more concise representation of the data while preserving its essential characteristics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eb36948-98bf-4f8d-8e3f-8995b8693908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daf2fe93-1333-49ab-9d2e-0a9ac73238de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price    rating  delivery_time\n",
       "0   0.00  0.833333           0.00\n",
       "1   0.50  0.000000           0.50\n",
       "2   0.25  1.000000           0.25\n",
       "3   1.00  0.277778           1.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'price': [10, 30, 20, 50],\n",
    "    'rating': [4.5, 3.0, 4.8, 3.5],\n",
    "    'delivery_time': [20, 40, 30, 60]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max scaling to the features\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1117ea5d-e7d6-4098-8f0e-b831c155ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code snippet demonstrates how to use Min-Max scaling on a DataFrame with features related to a food delivery service. \\nThe resulting df_scaled DataFrame will have all the numerical features scaled to the range [0, 1]. \\nThe scaled values can then be used in your recommendation system to ensure that each feature contributes proportionally to the recommendations.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This code snippet demonstrates how to use Min-Max scaling on a DataFrame with features related to a food delivery service. \n",
    "The resulting df_scaled DataFrame will have all the numerical features scaled to the range [0, 1]. \n",
    "The scaled values can then be used in your recommendation system to ensure that each feature contributes proportionally to the recommendations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c790c3f-1160-474a-ac89-71a5bd32210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\\nfeatures, such as company financial data and market trends. Explain how you would use PCA to reduce the\\ndimensionality of the dataset.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d80edba-c432-410b-ad0c-a876a097ee11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When dealing with a dataset with many features, such as company financial data and market trends in the context of predicting stock prices, dimensionality reduction techniques like Principal Component Analysis (PCA) can be beneficial. PCA helps in reducing the number of features while retaining the essential information and capturing the most significant patterns in the data. Here's how you would use PCA for dimensionality reduction in the context of predicting stock prices:\\n\\nUnderstand the Features:\\n\\nReview the features in your dataset, such as company financial metrics (e.g., revenue, profit, debt), market trends (e.g., moving averages, trading volumes), and any other relevant factors that might influence stock prices.\\n\\nStandardize the Data:\\nStandardize the features if they are measured in different units or have different scales. PCA is sensitive to the scale of the features, and standardization ensures that each feature contributes equally to the analysis.\\n\\nApply PCA:\\nUse PCA to transform the standardized data into its principal components. The principal components are linear combinations of the original features and are ordered by the amount of variance they capture.\\n\\nDetermine the Number of Components:\\nDecide on the number of principal components to retain. You can make this decision based on the cumulative explained variance. The cumulative explained variance plot can help you understand how much variance is retained as you increase the number of components.\\n\\nSelect Principal Components:\\nChoose the topk principal components, where  k is the desired reduced dimensionality. These k components will be used as the new features in your dataset.\\n\\nTransform the Data:\\nTransform the original data using the selected k principal components to obtain a reduced-dimensional representation of the dataset.\\n\\nHere's a simplified example using Python and scikit-learn:\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''When dealing with a dataset with many features, such as company financial data and market trends in the context of predicting stock prices, dimensionality reduction techniques like Principal Component Analysis (PCA) can be beneficial. PCA helps in reducing the number of features while retaining the essential information and capturing the most significant patterns in the data. Here's how you would use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "Understand the Features:\n",
    "\n",
    "Review the features in your dataset, such as company financial metrics (e.g., revenue, profit, debt), market trends (e.g., moving averages, trading volumes), and any other relevant factors that might influence stock prices.\n",
    "\n",
    "Standardize the Data:\n",
    "Standardize the features if they are measured in different units or have different scales. PCA is sensitive to the scale of the features, and standardization ensures that each feature contributes equally to the analysis.\n",
    "\n",
    "Apply PCA:\n",
    "Use PCA to transform the standardized data into its principal components. The principal components are linear combinations of the original features and are ordered by the amount of variance they capture.\n",
    "\n",
    "Determine the Number of Components:\n",
    "Decide on the number of principal components to retain. You can make this decision based on the cumulative explained variance. The cumulative explained variance plot can help you understand how much variance is retained as you increase the number of components.\n",
    "\n",
    "Select Principal Components:\n",
    "Choose the topk principal components, where  k is the desired reduced dimensionality. These k components will be used as the new features in your dataset.\n",
    "\n",
    "Transform the Data:\n",
    "Transform the original data using the selected k principal components to obtain a reduced-dimensional representation of the dataset.\n",
    "\n",
    "Here's a simplified example using Python and scikit-learn:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4abb138-66be-434c-8704-5dc5b4768da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ffba30-3f1e-43b8-bf43-59427888d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.57894737, -0.05263158,  0.47368421,  1.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "original_values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new range\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Min-Max scaling\n",
    "min_value = np.min(original_values)\n",
    "max_value = np.max(original_values)\n",
    "\n",
    "scaled_values = ((original_values - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min\n",
    "\n",
    "# Display the scaled values\n",
    "scaled_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f34437-6507-46e0-a122-8296d336e31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\\nFeature Extraction using PCA. How many principal components would you choose to retain, and why?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85314d20-4933-402d-b282-2f383cdec674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
