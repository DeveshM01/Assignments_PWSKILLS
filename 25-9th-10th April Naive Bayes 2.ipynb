{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cb3454-d514-49c8-a1ae-b14b485cf091",
   "metadata": {},
   "source": [
    "## Na√Øve bayes-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00395f65-958e-4c84-8cf8-b86ebad0bc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQ1. A company conducted a survey of its employees and found that 70% of the employees use the\\ncompany's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\\nprobability that an employee is a smoker given that he/she uses the health insurance plan?\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4e628b-dcdf-4be2-ae89-9232011655a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere's how to calculate the probability that an employee is a smoker given that he/she uses the health insurance plan:\\n\\nDefine the events:\\n\\nA: being a smoker\\nB: using the health insurance plan\\nGather the given information:\\n\\nP(B) = 0.7 (probability of using the health insurance plan)\\nP(A | B) = 0.4 (probability of being a smoker given using the health insurance plan)\\nApply Bayes' theorem:\\n\\nP(A | B) = P(B | A) * P(A) / P(B)\\n\\nWe're missing P(A), the prior probability of being a smoker in the entire employee population. Unfortunately, the provided information isn't sufficient to determine it.\\n\\nTherefore, we cannot calculate the exact probability of an employee being a smoker given that he/she uses the health insurance plan without knowing the overall smoking rate among employees (P(A)).\\n\\nIf you can provide the missing information about the general smoking rate in the company, I'd be happy to help you calculate the probability!\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here's how to calculate the probability that an employee is a smoker given that he/she uses the health insurance plan:\n",
    "\n",
    "Define the events:\n",
    "\n",
    "A: being a smoker\n",
    "B: using the health insurance plan\n",
    "Gather the given information:\n",
    "\n",
    "P(B) = 0.7 (probability of using the health insurance plan)\n",
    "P(A | B) = 0.4 (probability of being a smoker given using the health insurance plan)\n",
    "Apply Bayes' theorem:\n",
    "\n",
    "P(A | B) = P(B | A) * P(A) / P(B)\n",
    "\n",
    "We're missing P(A), the prior probability of being a smoker in the entire employee population. Unfortunately, the provided information isn't sufficient to determine it.\n",
    "\n",
    "Therefore, we cannot calculate the exact probability of an employee being a smoker given that he/she uses the health insurance plan without knowing the overall smoking rate among employees (P(A)).\n",
    "\n",
    "If you can provide the missing information about the general smoking rate in the company, I'd be happy to help you calculate the probability!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230db98d-8681-42e9-ab5a-5082af27ebaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee6b284-0dc4-4118-8a21-0071a82b9cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoth Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes classifier, but they differ in how they handle features and their assumptions about data distribution:\\n\\n1. Feature Representation:\\n\\nBernoulli Naive Bayes: Deals with binary features represented as 0 or 1 (e.g., word present/absent in a document, attribute present/absent in a data point). It counts the occurrences of each feature value (0 or 1) within each class.\\nMultinomial Naive Bayes: Handles discrete features with non-negative counts representing how many times each feature value appears (e.g., word frequency in a document, number of occurrences of an attribute value). It considers the distribution of feature counts within each class.\\n2. Data Distribution Assumptions:\\n\\nBernoulli Naive Bayes: Assumes each feature follows a Bernoulli distribution. This means the probability of a feature being 1 follows a binomial distribution with parameters n (number of trials) and p (probability of success).\\nMultinomial Naive Bayes: Assumes each feature follows a multinomial distribution. This means the probability distribution of different feature values within a class follows a multinomial distribution with parameters n (number of trials) and p_i (probability of each feature value).\\n3. Applications:\\n\\nBernoulli Naive Bayes: Often used for text classification with bag-of-words representation, spam filtering, and binary feature datasets.\\nMultinomial Naive Bayes: More suitable for text classification with word counts, document analysis, and datasets with discrete features with varied frequencies.\\n4. Advantages:\\n\\nBernoulli Naive Bayes: Simple and computationally efficient, handles sparse data well.\\nMultinomial Naive Bayes: More flexible for representing feature frequencies and distributions, often performs better than Bernoulli Naive Bayes for datasets with count features.\\n5. Disadvantages:\\n\\nBernoulli Naive Bayes: Ignores information about non-occurring features, might not be suitable for datasets with non-binary features.\\nMultinomial Naive Bayes: Can be less efficient for large datasets with high dimensionality.\\nChoosing the right type:\\n\\nThe choice between Bernoulli and Multinomial Naive Bayes depends on the nature of your data and features:\\n\\nIf your features are binary, Bernoulli Naive Bayes might be a good choice.\\nIf your features are discrete with count information, Multinomial Naive Bayes is often preferred.\\nConsider experiment with both models to determine the best option for your specific problem.\\nI hope this explanation clarifies the key differences between Bernoulli and Multinomial Naive Bayes! If you have any further questions about their applications or choosing the right type for your data, feel free to ask!\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes classifier, but they differ in how they handle features and their assumptions about data distribution:\n",
    "\n",
    "1. Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: Deals with binary features represented as 0 or 1 (e.g., word present/absent in a document, attribute present/absent in a data point). It counts the occurrences of each feature value (0 or 1) within each class.\n",
    "Multinomial Naive Bayes: Handles discrete features with non-negative counts representing how many times each feature value appears (e.g., word frequency in a document, number of occurrences of an attribute value). It considers the distribution of feature counts within each class.\n",
    "2. Data Distribution Assumptions:\n",
    "\n",
    "Bernoulli Naive Bayes: Assumes each feature follows a Bernoulli distribution. This means the probability of a feature being 1 follows a binomial distribution with parameters n (number of trials) and p (probability of success).\n",
    "Multinomial Naive Bayes: Assumes each feature follows a multinomial distribution. This means the probability distribution of different feature values within a class follows a multinomial distribution with parameters n (number of trials) and p_i (probability of each feature value).\n",
    "3. Applications:\n",
    "\n",
    "Bernoulli Naive Bayes: Often used for text classification with bag-of-words representation, spam filtering, and binary feature datasets.\n",
    "Multinomial Naive Bayes: More suitable for text classification with word counts, document analysis, and datasets with discrete features with varied frequencies.\n",
    "4. Advantages:\n",
    "\n",
    "Bernoulli Naive Bayes: Simple and computationally efficient, handles sparse data well.\n",
    "Multinomial Naive Bayes: More flexible for representing feature frequencies and distributions, often performs better than Bernoulli Naive Bayes for datasets with count features.\n",
    "5. Disadvantages:\n",
    "\n",
    "Bernoulli Naive Bayes: Ignores information about non-occurring features, might not be suitable for datasets with non-binary features.\n",
    "Multinomial Naive Bayes: Can be less efficient for large datasets with high dimensionality.\n",
    "Choosing the right type:\n",
    "\n",
    "The choice between Bernoulli and Multinomial Naive Bayes depends on the nature of your data and features:\n",
    "\n",
    "If your features are binary, Bernoulli Naive Bayes might be a good choice.\n",
    "If your features are discrete with count information, Multinomial Naive Bayes is often preferred.\n",
    "Consider experiment with both models to determine the best option for your specific problem.\n",
    "I hope this explanation clarifies the key differences between Bernoulli and Multinomial Naive Bayes! If you have any further questions about their applications or choosing the right type for your data, feel free to ask!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b8b645-8e25-478d-b16e-cca87f2d1934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ3. How does Bernoulli Naive Bayes handle missing values?\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4d9e64-50b8-4f8a-a6b0-d3ce0a523aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBernoulli Naive Bayes has a convenient way of handling missing values in your data: it simply ignores them! Here\\'s how it works:\\n\\nAssumptions: Remember that Bernoulli Naive Bayes assumes each feature follows a Bernoulli distribution with two possible values: 0 (absent) and 1 (present).\\nMissing values: When encountering a missing value for a feature, Bernoulli Naive Bayes treats it as an additional possible value alongside 0 and 1.\\nParameter estimation: During model training, Bernoulli Naive Bayes estimates separate parameters (probability of feature being 1) for each feature value, including the newly introduced \"missing\" value.\\nPrediction: For new data points with missing values, the same logic applies. The model calculates the likelihood of each class based on the observed features and treats the missing feature as its own category.\\nBenefits of this approach:\\n\\nSimplicity: Ignoring missing values keeps the implementation straightforward and computationally efficient.\\nRobustness: The model doesn\\'t rely on imputing missing values, which can be prone to biases and uncertainties.\\nFlexibility: This handling is agnostic to the reason for missingness, making it applicable to various scenarios.\\nHowever, there are some limitations to consider:\\n\\nInformation loss: Ignoring missing values discards potential information that could be valuable for prediction.\\nBias towards frequent values: If missing values are often associated with specific feature values, the model might be biased towards those values.\\nPerformance impact: Depending on the extent and distribution of missing values, model performance might suffer.\\nOverall, Bernoulli Naive Bayes offers a simple and robust way to handle missing values, though it\\'s important to be aware of potential limitations and consider alternative techniques if necessary.\\n\\nHere are some additional options for handling missing values with Bernoulli Naive Bayes:\\n\\nImputing missing values with the mean or median of the feature distribution.\\nUsing feature engineering techniques to encode missing values as a separate category.\\nEmploying more sophisticated missing value imputation algorithms like K-Nearest Neighbors or Matrix Factorization.\\nRemember, the best approach depends on the specific characteristics of your data and the context of your problem.\\n\\nFeel free to ask if you have further questions about missing value handling or specific methods you might be considering!\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bernoulli Naive Bayes has a convenient way of handling missing values in your data: it simply ignores them! Here's how it works:\n",
    "\n",
    "Assumptions: Remember that Bernoulli Naive Bayes assumes each feature follows a Bernoulli distribution with two possible values: 0 (absent) and 1 (present).\n",
    "Missing values: When encountering a missing value for a feature, Bernoulli Naive Bayes treats it as an additional possible value alongside 0 and 1.\n",
    "Parameter estimation: During model training, Bernoulli Naive Bayes estimates separate parameters (probability of feature being 1) for each feature value, including the newly introduced \"missing\" value.\n",
    "Prediction: For new data points with missing values, the same logic applies. The model calculates the likelihood of each class based on the observed features and treats the missing feature as its own category.\n",
    "Benefits of this approach:\n",
    "\n",
    "Simplicity: Ignoring missing values keeps the implementation straightforward and computationally efficient.\n",
    "Robustness: The model doesn't rely on imputing missing values, which can be prone to biases and uncertainties.\n",
    "Flexibility: This handling is agnostic to the reason for missingness, making it applicable to various scenarios.\n",
    "However, there are some limitations to consider:\n",
    "\n",
    "Information loss: Ignoring missing values discards potential information that could be valuable for prediction.\n",
    "Bias towards frequent values: If missing values are often associated with specific feature values, the model might be biased towards those values.\n",
    "Performance impact: Depending on the extent and distribution of missing values, model performance might suffer.\n",
    "Overall, Bernoulli Naive Bayes offers a simple and robust way to handle missing values, though it's important to be aware of potential limitations and consider alternative techniques if necessary.\n",
    "\n",
    "Here are some additional options for handling missing values with Bernoulli Naive Bayes:\n",
    "\n",
    "Imputing missing values with the mean or median of the feature distribution.\n",
    "Using feature engineering techniques to encode missing values as a separate category.\n",
    "Employing more sophisticated missing value imputation algorithms like K-Nearest Neighbors or Matrix Factorization.\n",
    "Remember, the best approach depends on the specific characteristics of your data and the context of your problem.\n",
    "\n",
    "Feel free to ask if you have further questions about missing value handling or specific methods you might be considering!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c070391-6426-42d5-a00b-7b701fa2a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ4. Can Gaussian Naive Bayes be used for multi-class classification?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a55ecd-8789-43bf-aec2-9da8bd03ee71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYes, Gaussian Naive Bayes can be effectively used for multi-class classification problems. Here\\'s how it achieves this:\\n\\n1. Assumptions:\\n\\nGaussian Naive Bayes assumes that each feature within each class follows a Gaussian (normal) distribution.\\nIt also makes the \"naive\" assumption that features are independent of each other, even though this might not be entirely true in real-world data.\\n2. Training:\\n\\nDuring training, the model estimates the mean and standard deviation of each feature within each class.\\nIt uses these parameters to create a probability density function (PDF) for each feature in each class.\\n3. Prediction:\\n\\nGiven a new data point with unknown class membership, the model calculates the likelihood of it belonging to each possible class using the PDFs.\\nIt then applies Bayes\\' theorem to combine these likelihoods with prior probabilities of each class (if available) to determine the posterior probabilities of each class.\\nThe class with the highest posterior probability is assigned as the prediction.\\nIllustration:\\n\\nImage of Gaussian Naive Bayes multiclass classificationOpens in a new window\\npriyansh32.hashnode.dev\\nGaussian Naive Bayes multiclass classification\\nExample:\\n\\nImagine a multi-class classification problem with three classes (A, B, C) and two features (x1, x2). Gaussian Naive Bayes would create three Gaussian distributions for each feature, one for each class. When a new data point arrives, it calculates three likelihood scores (one for each class) based on how well the data point fits those Gaussian distributions. The class with the highest likelihood score wins the prediction.\\n\\nKey Points:\\n\\nGaussian Naive Bayes is a simple and efficient algorithm for multi-class classification, often performing well even with large datasets.\\nIt\\'s essential to ensure your features are approximately normally distributed or consider data transformation techniques if necessary.\\nBe aware of the model\\'s \"naive\" assumption of feature independence, which might affect performance if strong feature correlations exist.\\nIn essence, Gaussian Naive Bayes extends its capabilities to multi-class problems by constructing multiple Gaussian distributions for each feature, one for each possible class, and using them to evaluate the likelihoods of a new data point belonging to each class.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Yes, Gaussian Naive Bayes can be effectively used for multi-class classification problems. Here's how it achieves this:\n",
    "\n",
    "1. Assumptions:\n",
    "\n",
    "Gaussian Naive Bayes assumes that each feature within each class follows a Gaussian (normal) distribution.\n",
    "It also makes the \"naive\" assumption that features are independent of each other, even though this might not be entirely true in real-world data.\n",
    "2. Training:\n",
    "\n",
    "During training, the model estimates the mean and standard deviation of each feature within each class.\n",
    "It uses these parameters to create a probability density function (PDF) for each feature in each class.\n",
    "3. Prediction:\n",
    "\n",
    "Given a new data point with unknown class membership, the model calculates the likelihood of it belonging to each possible class using the PDFs.\n",
    "It then applies Bayes' theorem to combine these likelihoods with prior probabilities of each class (if available) to determine the posterior probabilities of each class.\n",
    "The class with the highest posterior probability is assigned as the prediction.\n",
    "Illustration:\n",
    "\n",
    "Image of Gaussian Naive Bayes multiclass classificationOpens in a new window\n",
    "priyansh32.hashnode.dev\n",
    "Gaussian Naive Bayes multiclass classification\n",
    "Example:\n",
    "\n",
    "Imagine a multi-class classification problem with three classes (A, B, C) and two features (x1, x2). Gaussian Naive Bayes would create three Gaussian distributions for each feature, one for each class. When a new data point arrives, it calculates three likelihood scores (one for each class) based on how well the data point fits those Gaussian distributions. The class with the highest likelihood score wins the prediction.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Gaussian Naive Bayes is a simple and efficient algorithm for multi-class classification, often performing well even with large datasets.\n",
    "It's essential to ensure your features are approximately normally distributed or consider data transformation techniques if necessary.\n",
    "Be aware of the model's \"naive\" assumption of feature independence, which might affect performance if strong feature correlations exist.\n",
    "In essence, Gaussian Naive Bayes extends its capabilities to multi-class problems by constructing multiple Gaussian distributions for each feature, one for each possible class, and using them to evaluate the likelihoods of a new data point belonging to each class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8b4495-9baa-48f3-b107-271f4a48d09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5. Assignment:\\nData preparation:\\nDownload the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\\ndatasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\\nis spam or not based on several input features.\\nImplementation:\\nImplement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\\nscikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\\ndataset. You should use the default hyperparameters for each classifier.\\nResults:\\nReport the following performance metrics for each classifier:\\nAccuracy\\nPrecision\\nRecall\\nF1 score\\nDiscussion:\\nDiscuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\\nthe case? Are there any limitations of Naive Bayes that you observed?\\nConclusion:\\nSummarise your findings and provide some suggestions for future work.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc0186c-a5c0-49e3-ba78-6c55be494c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport pandas as pd\\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Load the dataset\\ndata = pd.read_csv(\"spambase.data.csv\", header=None)  # Assuming CSV format\\n\\n# Separate features and target\\nX = data.iloc[:, :-1]\\ny = data.iloc[:, -1]\\n\\n# Define the classifiers\\nmodels = [BernoulliNB(), MultinomialNB(), GaussianNB()]\\n\\n# Evaluate each classifier using 10-fold cross-validation\\nfor model in models:\\n    scores = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\\n    print(f\"{model.__class__.__name__} Accuracy: {scores.mean():.4f}\")\\n\\n    # Calculate other metrics\\n    y_pred = cross_val_predict(model, X, y, cv=10)\\n    precision = precision_score(y, y_pred)\\n    recall = recall_score(y, y_pred)\\n    f1 = f1_score(y, y_pred)\\n    print(f\"Precision: {precision:.4f}\")\\n    print(f\"Recall: {recall:.4f}\")\\n    print(f\"F1-score: {f1:.4f}\")\\n\\n# Discussion and Conclusion (replace with your actual results and insights)\\n# - Analyze which model performs best and why, considering data characteristics and model assumptions.\\n# - Discuss any limitations observed, such as the feature independence assumption.\\n# - Suggest future directions, including hyperparameter tuning, feature engineering, and alternative algorithms.\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data.csv\", header=None)  # Assuming CSV format\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Define the classifiers\n",
    "models = [BernoulliNB(), MultinomialNB(), GaussianNB()]\n",
    "\n",
    "# Evaluate each classifier using 10-fold cross-validation\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n",
    "    print(f\"{model.__class__.__name__} Accuracy: {scores.mean():.4f}\")\n",
    "\n",
    "    # Calculate other metrics\n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Discussion and Conclusion (replace with your actual results and insights)\n",
    "# - Analyze which model performs best and why, considering data characteristics and model assumptions.\n",
    "# - Discuss any limitations observed, such as the feature independence assumption.\n",
    "# - Suggest future directions, including hyperparameter tuning, feature engineering, and alternative algorithms.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6b1d3-2236-4abc-9d2f-06c280785851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
