{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb002b39-f6d3-4545-8a4b-4b4e5e2bf64e",
   "metadata": {},
   "source": [
    "## Support Vector Machines-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a335579-681b-4bd4-959c-5b9ae4d3609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ1. What is the relationship between polynomial functions and kernel functions in machine learning\\nalgorithms?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a15164-b06e-45b2-866d-8eeedd4b33cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI\\'ll provide a comprehensive explanation of the relationship between polynomial functions and kernel functions in machine learning algorithms:\\n\\nPolynomial Functions:\\n\\nMathematical expressions involving variables raised to non-negative integer powers (e.g., x^2, x^3, etc.).\\nIn machine learning, used to model non-linear relationships between features and target variables.\\nCan capture more complex patterns than linear functions, which are limited to straight lines.\\nKernel Functions:\\n\\nMathematical functions that compute similarity or \"inner product\" between data points in a high-dimensional feature space.\\nEnable algorithms to work in high-dimensional spaces without explicitly transforming the data.\\nUsed to project data into a higher-dimensional space where linear separation may be easier.\\nRelationship:\\n\\nPolynomial functions are a specific type of kernel function.\\nA polynomial kernel function calculates the similarity between data points using a polynomial of their features.\\nThis allows algorithms like SVMs to capture non-linear relationships in the original feature space, even though they are designed for linear separation.\\nExample:\\n\\nPolynomial kernel of degree 2:\\nK(x, y) = (x^T * y + 1)^2\\nCalculates the square of the dot product between two vectors, effectively mapping them to a higher-dimensional space where non-linear relationships can be captured linearly.\\nKey Points:\\n\\nKernel functions provide a powerful tool for handling non-linear data in machine learning.\\nPolynomial kernels are a versatile option for capturing non-linear relationships.\\nOther popular kernel functions include Gaussian kernels (RBF), which measure similarity based on radial distance, and sigmoid kernels, often used in neural networks.\\nThe choice of kernel function depends on the specific problem and data characteristics.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "I'll provide a comprehensive explanation of the relationship between polynomial functions and kernel functions in machine learning algorithms:\n",
    "\n",
    "Polynomial Functions:\n",
    "\n",
    "Mathematical expressions involving variables raised to non-negative integer powers (e.g., x^2, x^3, etc.).\n",
    "In machine learning, used to model non-linear relationships between features and target variables.\n",
    "Can capture more complex patterns than linear functions, which are limited to straight lines.\n",
    "Kernel Functions:\n",
    "\n",
    "Mathematical functions that compute similarity or \"inner product\" between data points in a high-dimensional feature space.\n",
    "Enable algorithms to work in high-dimensional spaces without explicitly transforming the data.\n",
    "Used to project data into a higher-dimensional space where linear separation may be easier.\n",
    "Relationship:\n",
    "\n",
    "Polynomial functions are a specific type of kernel function.\n",
    "A polynomial kernel function calculates the similarity between data points using a polynomial of their features.\n",
    "This allows algorithms like SVMs to capture non-linear relationships in the original feature space, even though they are designed for linear separation.\n",
    "Example:\n",
    "\n",
    "Polynomial kernel of degree 2:\n",
    "K(x, y) = (x^T * y + 1)^2\n",
    "Calculates the square of the dot product between two vectors, effectively mapping them to a higher-dimensional space where non-linear relationships can be captured linearly.\n",
    "Key Points:\n",
    "\n",
    "Kernel functions provide a powerful tool for handling non-linear data in machine learning.\n",
    "Polynomial kernels are a versatile option for capturing non-linear relationships.\n",
    "Other popular kernel functions include Gaussian kernels (RBF), which measure similarity based on radial distance, and sigmoid kernels, often used in neural networks.\n",
    "The choice of kernel function depends on the specific problem and data characteristics.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68687881-416e-422f-933d-84bbb7e52cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805f5ebe-908b-4e69-9d55-2cf2f48ac278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, datasets, model_selection\n",
    "\n",
    "# Load Iris dataset (example)\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "model = svm.SVC(kernel='poly', degree=3, C=1.0)  # Adjust degree and C as needed\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93eb7bd2-78c6-472a-982c-1f8c6d4c137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ3. How does increasing the value of epsilon affect the number of support vectors in SVR?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6899facb-44ca-482b-8f56-397ae0210948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere\\'s how increasing the value of epsilon affects the number of support vectors in SVR:\\n\\nUnderstanding Epsilon:\\n\\nIn SVR, epsilon (ε) defines a margin of tolerance around the decision boundary.\\nData points within this margin are considered \"correctly\" predicted, even if they don\\'t fall exactly on the hyperplane.\\nIt controls the trade-off between fitting the training data precisely and allowing some flexibility for generalization.\\nEffect on Support Vectors:\\n\\nIncreasing epsilon generally decreases the number of support vectors.\\nHere\\'s why:\\nA larger epsilon creates a wider margin of tolerance.\\nMore data points fall within this margin, so they don\\'t contribute as support vectors.\\nSupport vectors are those points that directly influence the position and orientation of the decision boundary.\\nA wider margin accommodates more points without violating the ε-insensitive loss, reducing the need for support vectors.\\nVisualization:\\n\\nImagine a wider tube around the decision boundary (larger ε).\\nPoints within this tube don\\'t actively push or pull the boundary, so they aren\\'t support vectors.\\nA narrower tube (smaller ε) requires more points to define its edges, resulting in more support vectors.\\nKey Points:\\n\\nLarger epsilon = wider margin = fewer support vectors\\nSmaller epsilon = narrower margin = more support vectors\\nThe choice of epsilon affects model complexity and generalization.\\nToo large an epsilon might lead to underfitting (too simple a model).\\nToo small an epsilon might lead to overfitting (overly complex model).\\nIt\\'s crucial to experiment with different epsilon values to find the optimal balance for your specific problem.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "Understanding Epsilon:\n",
    "\n",
    "In SVR, epsilon (ε) defines a margin of tolerance around the decision boundary.\n",
    "Data points within this margin are considered \"correctly\" predicted, even if they don't fall exactly on the hyperplane.\n",
    "It controls the trade-off between fitting the training data precisely and allowing some flexibility for generalization.\n",
    "Effect on Support Vectors:\n",
    "\n",
    "Increasing epsilon generally decreases the number of support vectors.\n",
    "Here's why:\n",
    "A larger epsilon creates a wider margin of tolerance.\n",
    "More data points fall within this margin, so they don't contribute as support vectors.\n",
    "Support vectors are those points that directly influence the position and orientation of the decision boundary.\n",
    "A wider margin accommodates more points without violating the ε-insensitive loss, reducing the need for support vectors.\n",
    "Visualization:\n",
    "\n",
    "Imagine a wider tube around the decision boundary (larger ε).\n",
    "Points within this tube don't actively push or pull the boundary, so they aren't support vectors.\n",
    "A narrower tube (smaller ε) requires more points to define its edges, resulting in more support vectors.\n",
    "Key Points:\n",
    "\n",
    "Larger epsilon = wider margin = fewer support vectors\n",
    "Smaller epsilon = narrower margin = more support vectors\n",
    "The choice of epsilon affects model complexity and generalization.\n",
    "Too large an epsilon might lead to underfitting (too simple a model).\n",
    "Too small an epsilon might lead to overfitting (overly complex model).\n",
    "It's crucial to experiment with different epsilon values to find the optimal balance for your specific problem.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98a89a7-e410-4a65-866c-a6b9f598ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\\naffect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\\nand provide examples of when you might want to increase or decrease its value?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a02261-46a9-42c7-a482-3df8e12f73c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere's how the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR):\\n\\n1. Kernel Function:\\n\\nPurpose: Transforms data into a higher-dimensional space where linear separation is possible, enabling SVR to capture non-linear relationships.\\nCommon Kernels:\\nLinear: Suitable for linearly separable data.\\nPolynomial: Captures non-linear relationships of a specific degree.\\nRBF (Radial Basis Function): Maps data to an infinite-dimensional space, often effective for complex patterns.\\nWhen to Choose:\\nLinear: When data is linearly separable.\\nPolynomial: When data exhibits polynomial relationships.\\nRBF: When data has complex, non-linear patterns.\\n2. C Parameter (Regularization Parameter):\\n\\nPurpose: Controls the trade-off between margin maximization and training error minimization.\\nEffect:\\nLarger C: Emphasizes minimizing training error, potentially leading to overfitting.\\nSmaller C: Allows for a wider margin, potentially improving generalization.\\nWhen to Adjust:\\nIncrease C: If model underfits or exhibits high bias.\\nDecrease C: If model overfits or has high variance.\\n3. Epsilon Parameter:\\n\\nPurpose: Defines the tolerance for errors within the margin of the decision boundary.\\nEffect:\\nLarger epsilon: Creates a wider margin, potentially leading to underfitting.\\nSmaller epsilon: Enforces stricter fitting to the data, potentially leading to overfitting.\\nWhen to Adjust:\\nIncrease epsilon: If model is too sensitive to noise or outliers.\\nDecrease epsilon: If model is too simple and misses important patterns.\\n4. Gamma Parameter (for non-linear kernels like RBF):\\n\\nPurpose: Controls the influence of individual training samples on the decision boundary.\\nEffect:\\nLarger gamma: Considers data points closer to the decision boundary more influential, leading to a more complex model.\\nSmaller gamma: Smoother decision boundary, potentially less sensitive to noise.\\nWhen to Adjust:\\nIncrease gamma: If data has clear, well-defined non-linear patterns.\\nDecrease gamma: If data is noisy or patterns are less distinct.\\nKey Points:\\n\\nExperimentation with different kernel functions and parameter values is crucial for optimizing SVR performance.\\nUnderstanding the role of each parameter helps guide model selection and tuning.\\nNo fixed rules for parameter settings; ideal values depend on specific dataset and problem characteristics.\\nCross-validation is essential for evaluating model performance and avoiding overfitting.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here's how the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR):\n",
    "\n",
    "1. Kernel Function:\n",
    "\n",
    "Purpose: Transforms data into a higher-dimensional space where linear separation is possible, enabling SVR to capture non-linear relationships.\n",
    "Common Kernels:\n",
    "Linear: Suitable for linearly separable data.\n",
    "Polynomial: Captures non-linear relationships of a specific degree.\n",
    "RBF (Radial Basis Function): Maps data to an infinite-dimensional space, often effective for complex patterns.\n",
    "When to Choose:\n",
    "Linear: When data is linearly separable.\n",
    "Polynomial: When data exhibits polynomial relationships.\n",
    "RBF: When data has complex, non-linear patterns.\n",
    "2. C Parameter (Regularization Parameter):\n",
    "\n",
    "Purpose: Controls the trade-off between margin maximization and training error minimization.\n",
    "Effect:\n",
    "Larger C: Emphasizes minimizing training error, potentially leading to overfitting.\n",
    "Smaller C: Allows for a wider margin, potentially improving generalization.\n",
    "When to Adjust:\n",
    "Increase C: If model underfits or exhibits high bias.\n",
    "Decrease C: If model overfits or has high variance.\n",
    "3. Epsilon Parameter:\n",
    "\n",
    "Purpose: Defines the tolerance for errors within the margin of the decision boundary.\n",
    "Effect:\n",
    "Larger epsilon: Creates a wider margin, potentially leading to underfitting.\n",
    "Smaller epsilon: Enforces stricter fitting to the data, potentially leading to overfitting.\n",
    "When to Adjust:\n",
    "Increase epsilon: If model is too sensitive to noise or outliers.\n",
    "Decrease epsilon: If model is too simple and misses important patterns.\n",
    "4. Gamma Parameter (for non-linear kernels like RBF):\n",
    "\n",
    "Purpose: Controls the influence of individual training samples on the decision boundary.\n",
    "Effect:\n",
    "Larger gamma: Considers data points closer to the decision boundary more influential, leading to a more complex model.\n",
    "Smaller gamma: Smoother decision boundary, potentially less sensitive to noise.\n",
    "When to Adjust:\n",
    "Increase gamma: If data has clear, well-defined non-linear patterns.\n",
    "Decrease gamma: If data is noisy or patterns are less distinct.\n",
    "Key Points:\n",
    "\n",
    "Experimentation with different kernel functions and parameter values is crucial for optimizing SVR performance.\n",
    "Understanding the role of each parameter helps guide model selection and tuning.\n",
    "No fixed rules for parameter settings; ideal values depend on specific dataset and problem characteristics.\n",
    "Cross-validation is essential for evaluating model performance and avoiding overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ab69c9-0584-4837-9606-eb9c4de54111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5. Assignment:\\nL Import the necessary libraries and load the dataseg\\nL Split the dataset into training and testing setZ\\nL Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\\nL Create an instance of the SVC classifier and train it on the training datW\\nL hse the trained classifier to predict the labels of the testing datW\\nL Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\\nprecision, recall, F1-scoreK\\nL Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\\nimprove its performanc_\\nL Train the tuned classifier on the entire dataseg\\nL Save the trained classifier to a file for future use.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301f6023-b643-436d-850b-c7620bc1fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svm_model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "X = iris.iloc[:, :-1]\n",
    "y = iris.iloc[:, -1]\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Preprocess the data (standard scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Create and train the initial SVC model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate initial performance\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Initial accuracy:\", accuracy)\n",
    "\n",
    "# 6. Tune hyperparameters\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'poly', 'rbf']}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 7. Train tuned model on entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# 8. Save the tuned model\n",
    "dump(best_model, 'tuned_svm_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28d1f1-26ab-43f1-9fb6-ccea24d53316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
